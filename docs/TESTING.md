# COMPREHENSIVE TESTING GUIDE
## Hedge Fund Microstructure Analysis System

ðŸ¦ **CRITICAL**: This document covers all testing procedures for the production hedge fund trading system. Money is on the line - follow all testing protocols exactly.

---

## Table of Contents

1. [Test Infrastructure Overview](#test-infrastructure-overview)
2. [Core System Tests](#core-system-tests)
3. [Data Quality & Integrity Tests](#data-quality--integrity-tests)
4. [UI & User Experience Tests](#ui--user-experience-tests)
5. [Performance & Load Testing](#performance--load-testing)
6. [Test Execution Commands](#test-execution-commands)
7. [CI/CD Integration](#cicd-integration)
8. [Troubleshooting](#troubleshooting)

---

## Test Infrastructure Overview

### Test Categories

| Category | Purpose | Location | Critical Level |
|----------|---------|----------|----------------|
| **Core System** | Algorithm validation, pipeline integrity | `tests/` | ðŸ”´ Critical |
| **Data Sanity** | Data quality, consistency, integrity | `tests/data_sanity/` | ðŸ”´ Critical |
| **UI/UX** | Dashboard functionality, user workflows | `tests/ui/` | ðŸŸ¡ High |
| **Performance** | Speed, resource usage, scalability | Throughout | ðŸŸ  Medium |

### Test Dependencies

```bash
# Core testing dependencies
pytest>=8.0.0
pytest-cov>=4.0.0
pytest-django>=4.0.0
playwright>=1.40.0

# Specialized testing tools
redis>=4.0.0
clickhouse-connect>=0.6.0
kafka-python>=2.0.0
requests>=2.28.0
```

---

## Core System Tests

### Microstructure Analysis Tests
**Location**: `tests/microstructure_analysis/`
**Purpose**: Validate L2 order book processing and signal detection algorithms

```bash
# Run all microstructure tests
pytest tests/microstructure_analysis/ -v

# Test specific signal detection algorithms
pytest tests/microstructure_analysis/test_signal_detection.py -v

# GPU-accelerated tests (requires CUDA)
pytest tests/microstructure_analysis/gpu/ -v -m gpu
```

**Key Test Areas**:
- Order book reconstruction accuracy
- Signal detection algorithm precision
- CUDA kernel validation with CPU fallback
- Feature calculation performance benchmarks

### Backtesting Engine Tests
**Location**: `tests/backtesting/`
**Purpose**: Validate event-driven backtesting engine with realistic market conditions

```bash
# Run backtesting validation suite
pytest tests/backtesting/ -v

# Test strategy execution
pytest tests/backtesting/test_strategy_execution.py -v

# Performance benchmarking
pytest tests/backtesting/test_performance.py -v
```

**Key Test Areas**:
- Historical data replay accuracy
- Strategy execution logic
- Performance metrics calculation
- Risk management validation

### Data Ingestion Tests
**Location**: `tests/ingestion/`
**Purpose**: Validate IQFeed client, Kafka producers, and data pipeline

```bash
# Run ingestion pipeline tests
pytest tests/ingestion/ -v

# Test IQFeed connectivity (requires IQConnect.exe)
pytest tests/ingestion/test_iqfeed_client.py -v

# Test Kafka integration
pytest tests/ingestion/test_kafka_integration.py -v
```

**Key Test Areas**:
- IQFeed connection stability
- Kafka message publishing reliability
- Data format validation
- Pipeline latency measurement

---

## Data Quality & Integrity Tests

### Historical Data Quality Validation
**Location**: `tests/data_sanity/test_historical_data_quality.py`
**Purpose**: Comprehensive validation of historical market data integrity

```bash
# Run historical data validation
pytest tests/data_sanity/test_historical_data_quality.py -v
```

**Validation Checks**:
- âœ… **File Structure**: Naming conventions, missing pairs, file sizes
- âœ… **Tick Data Integrity**: Price ranges, timestamp ordering, volume consistency
- âœ… **L1 Data Integrity**: Bid/ask relationships, spread consistency
- âœ… **Cross-Validation**: Timestamp alignment between tick and L1 data

**Test Results**: Validates 448,534+ records across 10 symbols and 3 dates

### Real-Time Data Consistency
**Location**: `tests/data_sanity/test_realtime_data_consistency.py`
**Purpose**: Validate real-time data flow consistency and pipeline health

```bash
# Run real-time consistency checks
pytest tests/data_sanity/test_realtime_data_consistency.py -v
```

**Consistency Checks**:
- âœ… **Redis Data Freshness**: System metrics, trading data, alert states
- âœ… **Kafka Message Consistency**: Format validation, timestamp checking
- âœ… **Pipeline Latency**: End-to-end latency measurement
- âœ… **System Resource Monitoring**: CPU, memory, disk utilization

### Message Integrity Testing
**Location**: `tests/data_sanity/test_kafka_message_integrity.py`
**Purpose**: Comprehensive Kafka message integrity and reliability validation

```bash
# Run Kafka message integrity tests
pytest tests/data_sanity/test_kafka_message_integrity.py -v
```

**Integrity Checks**:
- âœ… **Topic Configuration**: Replication factors, partition counts
- âœ… **Message Ordering**: Within-partition ordering validation  
- âœ… **Message Durability**: Delivery guarantees, persistence testing
- âœ… **Deduplication**: Duplicate message detection and handling

### Database Consistency Validation
**Location**: `tests/data_sanity/test_database_consistency.py`
**Purpose**: Cross-database consistency between Redis and ClickHouse

```bash
# Run database consistency tests
pytest tests/data_sanity/test_database_consistency.py -v
```

**Database Checks**:
- âœ… **Redis Data Integrity**: JSON validation, memory usage, performance
- âœ… **ClickHouse Data Integrity**: Schema validation, query performance
- âœ… **Cross-Database Consistency**: Data synchronization verification

### System Health Monitoring
**Location**: `tests/data_sanity/test_system_health_monitoring.py`
**Purpose**: Comprehensive system health and resource validation

```bash
# Run system health monitoring tests
pytest tests/data_sanity/test_system_health_monitoring.py -v
```

**Health Checks**:
- âœ… **System Resources**: CPU, memory, disk, network monitoring
- âœ… **Service Connectivity**: Kafka, Redis, ClickHouse, Zookeeper
- âœ… **Performance Metrics**: Response times, throughput, error rates
- âœ… **File System Health**: Critical paths, log files, data directories

---

## UI & User Experience Tests

### Dashboard Functionality Tests
**Location**: `tests/ui/test_django_playwright_comprehensive.py`
**Purpose**: End-to-end Django dashboard functionality validation

```bash
# Run comprehensive dashboard tests
pytest tests/ui/test_django_playwright_comprehensive.py -v --browser=chromium

# Test with multiple browsers
pytest tests/ui/ -v --browser=firefox
pytest tests/ui/ -v --browser=webkit

# Debug with visual browser
pytest tests/ui/ -v --browser=chromium --headed --slowmo=1000
```

**Dashboard Test Coverage**:
- âœ… **Page Loading**: Title verification, content validation
- âœ… **System Metrics Display**: CPU, memory, latency, throughput
- âœ… **Trading API Status**: IQFeed, Interactive Brokers connectivity
- âœ… **Alert System**: Active alerts, critical state handling
- âœ… **Real-time Indicators**: Live data streams, update mechanisms
- âœ… **Responsive Design**: Cross-viewport compatibility

### API Integration Tests
**Location**: `tests/ui/test_api_integration_playwright.py`
**Purpose**: API endpoint testing through browser interactions

```bash
# Run API integration tests
pytest tests/ui/test_api_integration_playwright.py -v
```

**API Test Coverage**:
- âœ… **Health Endpoints**: Status validation, response time testing
- âœ… **JSON Response Validation**: Format checking, data integrity
- âœ… **Error Handling**: Graceful failure handling, user feedback
- âœ… **WebSocket Connections**: Real-time data streaming
- âœ… **AJAX Functionality**: Dynamic content updates

### Trading Workflow Tests
**Location**: `tests/ui/test_trading_workflow_playwright.py`
**Purpose**: End-to-end trading workflow validation

```bash
# Run trading workflow tests
pytest tests/ui/test_trading_workflow_playwright.py -v
```

**Workflow Test Coverage**:
- âœ… **Market Data Display**: Price feeds, volume data, quote validation
- âœ… **System Monitoring**: Health indicators, performance metrics
- âœ… **Interactive Elements**: Button functionality, navigation
- âœ… **Data Visualization**: Charts, tables, status indicators

---

## Performance & Load Testing

### Performance Benchmarks

| Component | Target Performance | Test Command |
|-----------|-------------------|--------------|
| **L2 Book Updates** | < 10Î¼s per update | `pytest tests/microstructure_analysis/test_performance.py` |
| **Feature Calculation** | < 100Î¼s for 5 signals | `pytest tests/microstructure_analysis/test_signal_performance.py` |
| **Kafka Latency** | < 5ms end-to-end | `pytest tests/data_sanity/test_realtime_data_consistency.py` |
| **Dashboard Load** | < 15s initial load | `pytest tests/ui/test_api_integration_playwright.py::TestAPIPerformanceIntegration` |
| **API Response** | < 5s endpoint response | `pytest tests/ui/ -k "performance"` |

### Load Testing

```bash
# Run comprehensive performance tests
pytest tests/ -v -k "performance"

# Run concurrent load tests
pytest tests/ui/test_api_integration_playwright.py::TestAPIPerformanceIntegration::test_concurrent_api_load -v

# GPU performance benchmarking
pytest tests/microstructure_analysis/gpu/ -v -m performance
```

---

## Test Execution Commands

### Quick Test Suites

```bash
# Run all critical tests (5-10 minutes)
pytest tests/microstructure_analysis/ tests/data_sanity/ -v

# Run data quality validation (2-3 minutes)
pytest tests/data_sanity/ -v

# Run UI functionality tests (3-5 minutes)
pytest tests/ui/test_django_playwright_comprehensive.py::TestHedgeFundDashboardUI -v
```

### Comprehensive Test Suites

```bash
# Run complete test suite (15-30 minutes)
pytest tests/ -v --cov=. --cov-report=html

# Run with specific markers
pytest tests/ -v -m "not slow"  # Skip slow tests
pytest tests/ -v -m "critical"  # Only critical tests
pytest tests/ -v -m "gpu"       # Only GPU tests
```

### Browser-Specific UI Tests

```bash
# Chromium (fastest, best for CI/CD)
pytest tests/ui/ -v --browser=chromium

# Firefox (good cross-browser validation)
pytest tests/ui/ -v --browser=firefox

# Safari/WebKit (iOS/macOS compatibility)
pytest tests/ui/ -v --browser=webkit

# Multi-browser testing
pytest tests/ui/ -v --browser=chromium --browser=firefox --browser=webkit
```

### Debug and Development

```bash
# Visual debugging with headed browser
pytest tests/ui/ -v --browser=chromium --headed

# Slow motion for detailed observation
pytest tests/ui/ -v --browser=chromium --headed --slowmo=2000

# Verbose output for troubleshooting
pytest tests/ -v -s --tb=long

# Run specific failing test
pytest tests/ui/test_django_playwright_comprehensive.py::TestHedgeFundDashboardUI::test_dashboard_loads_and_title_correct -v -s
```

---

## CI/CD Integration

### GitHub Actions Configuration

```yaml
# .github/workflows/test.yml
name: Comprehensive Test Suite

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        playwright install chromium
    
    - name: Start infrastructure
      run: |
        cd infrastructure
        docker-compose up -d
        sleep 30
    
    - name: Run core tests
      run: pytest tests/microstructure_analysis/ tests/backtesting/ -v
    
    - name: Run data sanity tests
      run: pytest tests/data_sanity/ -v
    
    - name: Run UI tests
      run: pytest tests/ui/ -v --browser=chromium
    
    - name: Generate coverage report
      run: pytest --cov=. --cov-report=html
```

### Pre-commit Hooks

```bash
# Install pre-commit hooks
pip install pre-commit
pre-commit install

# Run tests before every commit
cat > .pre-commit-config.yaml << EOF
repos:
  - repo: local
    hooks:
      - id: pytest-critical
        name: Run critical tests
        entry: pytest tests/data_sanity/ -x
        language: system
        always_run: true
EOF
```

---

## Troubleshooting

### Common Issues & Solutions

#### Django Server Startup Issues
```bash
# Check port availability
lsof -i tcp:8000

# Kill existing processes
pkill -f "manage.py runserver"

# Manual server start for debugging
cd realtime/ui
python manage.py runserver 127.0.0.1:8000 --noreload
```

#### Playwright Browser Issues
```bash
# Install browsers
playwright install

# Check browser installation
playwright install --dry-run

# Debug browser launch
pytest tests/ui/ -v --browser=chromium --headed --debug
```

#### Redis/ClickHouse Connection Issues
```bash
# Check Redis connection
redis-cli -h localhost -p 6380 ping

# Check ClickHouse connection
curl http://localhost:8123/ping

# Start infrastructure services
cd infrastructure
docker-compose up -d
```

#### GPU Test Issues
```bash
# Check CUDA availability
nvidia-smi
python -c "from numba import cuda; print(cuda.gpus)"

# Install CUDA toolkit for testing
conda install cudatoolkit=10.2
pip install numba==0.53.1
```

### Performance Debugging

```bash
# Profile test execution
pytest tests/ -v --profile

# Memory usage monitoring
pytest tests/ -v --memray

# Benchmark comparison
pytest tests/ -v --benchmark-only
```

### Test Data Issues

```bash
# Clear test data
python -c "import redis; r=redis.Redis(host='localhost', port=6380, db=1); r.flushdb()"

# Regenerate test data
pytest tests/data_sanity/test_historical_data_quality.py::TestHistoricalDataQuality::test_file_structure_integrity -v
```

---

## Test Results & Reporting

### Expected Test Results

| Test Suite | Expected Results | Execution Time |
|------------|------------------|----------------|
| **Core System Tests** | 31/31 passing | 5-8 minutes |
| **Data Sanity Tests** | 25/25 passing | 3-5 minutes |
| **UI Dashboard Tests** | 7/7 passing | 2-3 minutes |
| **API Integration Tests** | 10/10 passing | 4-6 minutes |
| **Performance Tests** | All benchmarks met | 2-4 minutes |

### Coverage Targets

- **Core Algorithms**: > 95% coverage
- **Data Pipeline**: > 90% coverage  
- **UI Components**: > 85% coverage
- **API Endpoints**: > 90% coverage

### Success Criteria

âœ… **All critical tests passing**
âœ… **No silent failures detected** 
âœ… **Performance benchmarks met**
âœ… **Data integrity validated**
âœ… **UI functionality confirmed**
âœ… **API endpoints operational**

---

## Conclusion

This comprehensive testing infrastructure ensures the hedge fund microstructure analysis system meets production-grade reliability standards. 

**ðŸ”¥ CRITICAL REMINDER**: Always run the full test suite before deploying to production. Money is on the line - test everything!

For additional support or questions about the testing infrastructure, refer to the main project documentation in `CLAUDE.md` or contact the development team.